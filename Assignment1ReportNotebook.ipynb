{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used tools\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Logistic Regression\n",
    "class LRegression():\n",
    "    #Attributes\n",
    "    _features = None\n",
    "    _targets = None\n",
    "    _weights = None\n",
    "    featt = None\n",
    "    tarr = None\n",
    "    \n",
    "    def __init__(self, feat,target):\n",
    "        #TARGET IS EXPECTED TO BE changed to 1d array of 1 and zeros\n",
    "        feat = np.insert(feat,0,1,axis=1) ##Adding x^0\n",
    "        self._features = feat\n",
    "        self._weights = np.zeros(self._features[0].size)\n",
    "        self._targets = target\n",
    "        self.tarr = self._targets.flatten()\n",
    "        self.featt = self._features\n",
    "\n",
    "    def fit(self, tData, corVec, lRate, itera): \n",
    "        for x in range(itera):\n",
    "            N = tData.shape[0]\n",
    "            grad = np.dot((corVec-self.sig(np.dot(tData,self._weights))),tData) \n",
    "            grad *= lRate\n",
    "            grad /= N\n",
    "            self._weights = np.add(self._weights,grad)\n",
    "\n",
    "\n",
    "    def predict(self,tSet):\n",
    "        return self._classify(self.sig(np.array(np.dot(tSet, self._weights),dtype=np.float32)))\n",
    "    \n",
    "    def cost(self,trueV,predV):\n",
    "        obs = trueV.shape[0]\n",
    "        \n",
    "        return ((-trueV*np.log(predV))-((1-trueV)*np.log(1-predV))).sum()/obs\n",
    "    def sig(self,r):\n",
    "        return 1.0/(1.0+np.exp(-(np.array(r,dtype=np.float32))))\n",
    "    \n",
    "    def printW(self):\n",
    "        return self._weights\n",
    "    def _classify(self,pred):\n",
    "        l = []\n",
    "        for x in pred:\n",
    "            if x >= 0.5:\n",
    "                l.append(1)\n",
    "            else:\n",
    "                l.append(0)\n",
    "        return np.array(l)\n",
    "        \n",
    "    def Accu_eval(self,pL,tL):\n",
    "        n=tL.size\n",
    "        return np.mean(pL == tL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of LDA\n",
    "class LDA():\n",
    "    _feat = None\n",
    "    _targ = None\n",
    "    _N = None\n",
    "    _p_one = None\n",
    "    _p_zero = None\n",
    "    _u_one = None\n",
    "    _u_zero = None\n",
    "    _E_one = None\n",
    "    _E_zero = None\n",
    "    _E = None\n",
    "    def __init__(self,features,targets):\n",
    "        self._feat = features\n",
    "        self._targ = targets\n",
    "        self._N = targets.size #Expecting 1d Array of labels\n",
    "        self._n_one = targets.sum()\n",
    "        self._n_zero = (self._N-self._targ.sum())\n",
    "        self._p_one = self._n_one/self._N\n",
    "        self._p_zero = self._n_zero/self._N\n",
    "        self._u_zero = 0\n",
    "        self._u_one = 0\n",
    "        for x in range(self._N): ##INDICATOR\n",
    "            if self._targ[x] == 1:\n",
    "                self._u_one += self._feat[x]\n",
    "            else:\n",
    "                self._u_zero += self._feat[x]\n",
    "        self._u_zero = self._u_zero/self._n_zero #Expecting 1/0 labels passed on\n",
    "        self._u_one = self._u_one/self._n_one\n",
    "        self._E_one =0\n",
    "        self._E_zero =0\n",
    "        for x in range(self._N):\n",
    "            if self._targ[x] == 1:\n",
    "                self._E_one += np.outer((self._feat[x] - self._u_one),(self._feat[x] - self._u_one))/(self._N -2)\n",
    "            else:\n",
    "                self._E_zero += np.outer((self._feat[x] - self._u_zero),(self._feat[x] - self._u_zero))/(self._N -2)\n",
    "        self._E = self._E_one + self._E_zero\n",
    "\n",
    "    def predict(self,features):\n",
    "        predResult = []\n",
    "        a = np.log(self._p_one/self._p_zero)\n",
    "        b = (np.dot(np.dot(self._u_one.T,np.linalg.inv(np.array(self._E,dtype=np.float32))),self._u_one)/2)\n",
    "        c = (np.dot(np.dot(self._u_zero.T,np.linalg.inv(np.array(self._E,dtype=np.float32))),self._u_zero)/2)\n",
    "        for x in range(features.shape[0]):\n",
    "            d = np.dot(np.dot(features[x].T,np.linalg.inv(np.array(self._E,dtype=np.float32))),(self._u_one-self._u_zero))\n",
    "\n",
    "            res = a-b+c+d\n",
    "            if res > 0:\n",
    "                predResult.append(1)\n",
    "            else:\n",
    "                predResult.append(0)\n",
    "            \n",
    "        return np.array(predResult)\n",
    "    \n",
    "    def Accu_eval(self,pL,tL):\n",
    "        n=tL.size\n",
    "        return np.mean(pL == tL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of 10-fold\n",
    "#Returns a (kfolds,m,n) mxn is the 2d data array\n",
    "class k_fold():\n",
    "    _foldSize = None\n",
    "    _returnArr = []\n",
    "    def __init__(self,data,fold=10):\n",
    "        #Expecting 2d np array\n",
    "        self._foldSize = np.floor(data.shape[0]/fold)\n",
    "        np.random.shuffle(data)\n",
    "        for x in range(fold-1): #Last fold will have an extra element\n",
    "            self._returnArr.append(data[int(self._foldSize*x):int(self._foldSize*(x+1)),:])\n",
    "        self._returnArr.append(data[int(self._foldSize*(fold-1)):int(self._foldSize*((fold-1)+1)),:])\n",
    "        self._returnArr =np.array(self._returnArr)\n",
    "    def returnFolds(self):\n",
    "        return self._returnArr\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression applied to Sonar dataset\n",
      "Learning rate set to: 0.3\n",
      "At k=0, accurcy = 0.65\n",
      "At k=1, accurcy = 0.85\n",
      "At k=2, accurcy = 0.7\n",
      "At k=3, accurcy = 0.8\n",
      "At k=4, accurcy = 0.7\n",
      "At k=5, accurcy = 0.7\n",
      "At k=6, accurcy = 0.85\n",
      "At k=7, accurcy = 0.95\n",
      "At k=8, accurcy = 0.65\n",
      "At k=9, accurcy = 0.75\n",
      "Logistic Regression applied to Parkinsons dataset\n",
      "Learning rate set to: 0.1\n",
      "At k=0, accurcy = 0.6\n",
      "At k=1, accurcy = 0.8\n",
      "At k=2, accurcy = 0.8\n",
      "At k=3, accurcy = 0.8\n",
      "At k=4, accurcy = 0.7\n",
      "At k=5, accurcy = 0.75\n",
      "At k=6, accurcy = 0.85\n",
      "At k=7, accurcy = 0.95\n",
      "At k=8, accurcy = 0.6\n",
      "At k=9, accurcy = 0.75\n"
     ]
    }
   ],
   "source": [
    "def applyToLR(kfold,lr): \n",
    "    print(\"Learning rate set to: \"+str(lr))\n",
    "    models = []\n",
    "    for k in range(10): # Running accuracy on each fold, with the rest combined to apply\n",
    "        train = None\n",
    "        train = np.delete(kfold,k,0)\n",
    "        newT = []\n",
    "        for x in train:\n",
    "            for y in x:\n",
    "                newT.append(y)\n",
    "        train = np.array(newT)\n",
    "        test = kfold[k]\n",
    "        tr_x = train[:,:-1] #Expecting last column to be Y\n",
    "        tr_y = train[:,-1:].flatten()\n",
    "        te_x = test[:,:-1] #Expecting last column to be Y\n",
    "        te_y = test[:,-1:].flatten()\n",
    "        x = LRegression(tr_x,tr_y)\n",
    "        x.fit(x.featt,x.tarr,lr,1000)\n",
    "        pred = x.predict(np.insert(te_x,0,1,axis=1))\n",
    "        acc = x.Accu_eval(pred,te_y)\n",
    "        models.append(x)\n",
    "        print(\"At k=\"+str(k)+\", accurcy = \"+str(acc)) \n",
    "    return models\n",
    "df = np.array(pd.read_csv('LReg/data/u/sonar.all-data', sep=',',header=None))\n",
    "dq = np.array(pd.read_csv('LReg/data/u/parkinsons.data', sep=',',header=0))\n",
    "dqq = np.delete(dq,0,1)\n",
    "dqq = np.delete(dqq,16,1)\n",
    "dqx = dq[:,17:18]\n",
    "dp = np.append(dqq,dqx,axis=1)\n",
    "\n",
    "\n",
    "print(\"Logistic Regression applied to Sonar dataset\")\n",
    "x = k_fold(df)\n",
    "x_models = applyToLR(x.returnFolds(),0.3)\n",
    "print(\"Logistic Regression applied to Parkinsons dataset\")\n",
    "y = k_fold(dp)\n",
    "y_models = applyToLR(x.returnFolds(),0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def applyToLDA(kfold,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
